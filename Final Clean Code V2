import json
import os
import re
import unicodedata
from typing import Dict, Tuple, List
from dateutil.parser import parse #pip install python-dateutil
from fuzzywuzzy import fuzz    #pip install python-dateutil fuzzywuzzy langdetect
from langdetect import detect # Requires: pip install langdetect
from concurrent.futures import ProcessPoolExecutor
import matplotlib.pyplot as plt #pip install matplotlib

CONFIG = {
    "WEIRD_CRITERIA": {"max_age_days": 30, "min_followers": 10, "score_threshold": 1.7},
    "spam_keywords": ["spam", "scam", "fake ticket", "buy now", "win free", "click here", "discount code", "special promo code", "limited offer", "free trial"],
    "spam_patterns": [
        r'https?://(t\.co|bit\.ly|tinyurl\.com)/\S+',  # Match only short URLs
        r'\b\d{1,2}%?\s*off\b'  # Match only "off" discounts
    ],
    "min_text_length_for_langdetect": 20
}

json_dir_path = "/Users/shibai/Documents/TUE study materials/TUE 2024/JBG030 DBL Data/dbl_data_2025"
output_dir_path = os.path.join(json_dir_path, "../twitter_data_final")
plots_dir_path = os.path.join(output_dir_path, "plots")
os.makedirs(output_dir_path, exist_ok=True)
os.makedirs(plots_dir_path, exist_ok=True)

#Regular expression patterns are used to identify airline companies mentioned in tweet text, even when users spell or format the names differently.
airline_variations = {
    'klm': [r'\bklm\b', r'\bk\.l\.m\.\b'],
    'airfrance': [r'\bairfrance\b', r'\bair\s*france\b', r'\bAF\b', r'\bAF\d{2,4}\b', r'vol\s+air\s+france'],
    'british_airways': [r'\bbritish_airways\b', r'\bbritish\s*airways\b', r'\bba\b'],
    'americanair': [r'\bamericanair\b', r'\bamerican\s*airlines\b', r'\baa\b'],
    'lufthansa': [r'\blufthansa\b', r'\bdeutsche\s*lufthansa\b', r'\bluftansa\b', r'\blufthanza\b', r'\bLH\b', r'\bLH\d{2,4}\b'],
    'airberlin': [r'\bairberlin\b', r'\bair\s*berlin\b'],
    'airberlin assist': [r'\bairberlin\s*assist\b'],
    'easyjet': [r'\beasyjet\b', r'\beasy\s*jet\b'],
    'ryanair': [r'\bryanair\b', r'\bryan\s*air\b'],
    'singaporeair': [r'\bsingaporeair\b', r'\bsingapore\s*airlines\b'],
    'qantas': [r'\bqantas\b', r'\bQF\b', r'\bQF\d{2,4}\b', r'flying\s+kangaroo'],
    'etihadairways': [r'\betihadairways\b', r'\betihad\s*airways\b'],
    'virginatlantic': [r'\bvirginatlantic\b', r'\bvirgin\s*atlantic\b']
}

#If the tweet language matches the airlineâ€™s native language, the weirdness score is reduced to allow higher tolerance.
AIRLINE_LANG_TOLERANCE = {'lufthansa': 'de', 'airfrance': 'fr', 'klm': 'nl', 'etihadairways': 'ar'}
airline_patterns = [re.compile(pattern, re.IGNORECASE) for patterns in airline_variations.values() for pattern in patterns]
AIRLINE_NAMES = list(airline_variations.keys())
FUZZY_THRESHOLDS = {name: 80 for name in AIRLINE_NAMES}
spam_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in CONFIG["spam_patterns"]]
AIRLINE_KEYWORDS = ['flight', 'airline', 'flew', 'flying', 'plane', 'aviation', 'airport']

# Airline official accounts and domains for spam exemption
Airline_names = [
    'KLM', 'AirFrance', 'British_Airways', 'AmericanAir', 'Lufthansa',
    'AirBerlin', 'AirBerlin assist', 'easyJet', 'RyanAir',
    'SingaporeAir', 'Qantas', 'EtihadAirways', 'VirginAtlantic'
]
AIRLINE_DOMAINS = [
    'klm.com', 'klm.nl', 'klm.co.uk', 'airfrance.com', 'airfrance.fr', 'airfrance.us',
    'britishairways.com', 'aa.com', 'lufthansa.com', 'lufthansa.de', 'book.lufthansa.com',
    'airberlin.com', 'easyjet.com', 'ryanair.com', 'singaporeair.com', 'qantas.com',
    'etihad.com', 'virginatlantic.com'
]

def clean_text(text: str, tweet: Dict = None, filename: str = None) -> Tuple[str, bool]:
    """Clean and normalize text, detect spam with exemptions for airline retweets and URLs."""
    if not text:
        return "", False
    text = unicodedata.normalize('NFKC', text.lower()).strip()
    text = ' '.join(text.split())
    spam_reasons = []

    # Check if tweet is a retweet from an airline official account
    is_airline_retweet = False
    if tweet and 'retweeted_status' in tweet:
        retweet_user = tweet['retweeted_status'].get('user', {}).get('screen_name', '')
        if retweet_user in AIRLINE_ACCOUNTS:
            is_airline_retweet = True
            spam_reasons.append(f"exempt: airline retweet from {retweet_user}")

    # Check if URLs are from airline domains
    is_airline_url = False
    if tweet and 'entities' in tweet and 'urls' in tweet['entities']:
        for url in tweet['entities']['urls']:
            expanded_url = url.get('expanded_url', '').lower()
            if expanded_url and any(domain in expanded_url for domain in AIRLINE_DOMAINS):
                is_airline_url = True
                spam_reasons.append(f"exempt: airline URL {expanded_url}")
                break

    # Apply spam checks only if not exempt
    if not (is_airline_retweet or is_airline_url):
        matched_keyword = next((k for k in CONFIG["spam_keywords"] if k in text), None)
        if matched_keyword:
            spam_reasons.append(f"keyword: {matched_keyword}")
        for pattern in spam_patterns:
            if pattern.search(text):
                spam_reasons.append(f"pattern: {pattern.pattern}")

    is_spam = bool([r for r in spam_reasons if not r.startswith('exempt')])
    return text, is_spam

def extract_full_text(tweet: Dict) -> str:
    """Extract full text, handling retweets."""
    source = tweet.get('retweeted_status', tweet)
    return source.get('full_text') or source.get('extended_tweet', {}).get('full_text') or source.get('text') or ''

def validate_tweet(tweet: Dict) -> Tuple[bool, str]:
    """Validate required tweet fields."""
    required_fields = ['created_at', 'id', 'text', 'user']
    missing = [f for f in required_fields if f not in tweet or tweet[f] is None]
    return (False, f"Missing fields: {', '.join(missing)}") if missing else (True, "")

def extract_relevant_info(tweet: Dict, filename: str = None) -> Tuple[Dict, None]:
    """Extract and clean tweet data"""
    text, is_spam = clean_text(extract_full_text(tweet), tweet, filename)
    urls = tweet.get('entities', {}).get('urls', [])
    cleaned_urls = [{'url': u.get('url'), 'display_url': u.get('display_url')} for u in urls]
    return {
        'created_at': tweet.get('created_at'),
        'id': tweet.get('id'),
        'text': text,
        'is_spam': is_spam,
        'lang': tweet.get('lang'),
        'retweet_count': tweet.get('retweeted_status', tweet).get('retweet_count', 0), #Use original tweet's counts to get accurate retweet and like numbers, not just the retweet itself
        'favorite_count': tweet.get('retweeted_status', tweet).get('favorite_count', 0), #Get favorite count from original tweet if this is a retweet
        'in_reply_to_status_id': tweet.get('in_reply_to_status_id'),
        'in_reply_to_user_id': tweet.get('in_reply_to_user_id'),
        'in_reply_to_screen_name': tweet.get('in_reply_to_screen_name'),
        'is_quote_status': tweet.get('is_quote_status'),
        'quote_count': tweet.get('quote_count'),
        'reply_count': tweet.get('reply_count'),
        'place': tweet.get('place'),
        'favorited': tweet.get('favorited'),
        'retweeted': tweet.get('retweeted'),
        'user': {
            'id': tweet.get('user', {}).get('id'),
            'screen_name': tweet.get('user', {}).get('screen_name'),
            'name': tweet.get('user', {}).get('name'),
            'followers_count': tweet.get('user', {}).get('followers_count'),
            'friends_count': tweet.get('user', {}).get('friends_count'),
            'favourites_count': tweet.get('user', {}).get('favourites_count'),
            'statuses_count': tweet.get('user', {}).get('statuses_count'),
            'verified': tweet.get('user', {}).get('verified'),
            'location': tweet.get('user', {}).get('location'),
            'time_zone': tweet.get('user', {}).get('time_zone'),
            'created_at': tweet.get('user', {}).get('created_at')
        },
        'entities': {
            'hashtags': tweet.get('entities', {}).get('hashtags'),
            'user_mentions': tweet.get('entities', {}).get('user_mentions'),
            'urls': cleaned_urls,
            'symbols': tweet.get('entities', {}).get('symbols')
        }
    }, None

def is_weird_account(user: Dict, tweet_created_at: str, tweet: Dict) -> Tuple[bool, List[str]]:
    """Check if account is suspicious..."""
    if not user or not tweet:
        return True, ["missing_user_info_or_tweet"]
    score, reasons = 0.0, []
    followers = user.get('followers_count', 0)
    friends = user.get('friends_count', 0)
    statuses = user.get('statuses_count', 0)
    screen_name = user.get('screen_name', '')
    verified = user.get('verified', False)
    description = user.get('description', '')
    profile_image = user.get('profile_image_url', '')

    if followers < CONFIG["WEIRD_CRITERIA"]['min_followers'] and statuses > 5000:
        score += 0.8
        reasons.append("low_followers_high_activity")
    if followers < CONFIG["WEIRD_CRITERIA"]['min_followers'] and friends > 500:
        score += 0.6 if not verified else 0.2
        reasons.append("spammy_follow_ratio")
    if statuses < 10:
        score += 0.5
        reasons.append("very_low_tweet_count")
    if re.fullmatch(r'[a-zA-Z]*\d{5,}', screen_name):
        score += 0.4
        reasons.append("autogen_screen_name")
    if not verified:
        score += 0.05
        reasons.append("not_verified")
    if not description:
        score += 0.3
        reasons.append("empty_profile_description")
    if profile_image and 'default_profile' in profile_image.lower():
        score += 0.5
        reasons.append("default_profile_image")

    created_at = user.get('created_at', '')
    if created_at and tweet_created_at:
        try:
            user_created = parse(created_at, fuzzy=False)
            tweet_created = parse(tweet_created_at, fuzzy=False)
            age_days = max((tweet_created - user_created).days, 1)
            if age_days < CONFIG["WEIRD_CRITERIA"]['max_age_days'] and statuses > 500 and followers < CONFIG["WEIRD_CRITERIA"]['min_followers']:
                score += 0.7
                reasons.append("new_active_no_followers")
            if statuses / age_days > 100:
                score += 0.4
                reasons.append("high_tweet_frequency")
        except (ValueError, TypeError):
            reasons.append("skipped_age_check_invalid_date")

    text = extract_full_text(tweet).lower()
    tweet_lang = tweet.get('lang', '')
    # Use tweet['lang'] if reliable, fallback to langdetect
    if not tweet_lang or tweet_lang == 'und':
        if len(text) >= CONFIG["min_text_length_for_langdetect"]:
            try:
                tweet_lang = detect(text)
            except Exception as e:
                print(f"LangDetect failed for Tweet ID {tweet.get('id', 'unknown')}: {e}")
                tweet_lang = ''

    for airline, lang_code in AIRLINE_LANG_TOLERANCE.items():
        if airline in text and tweet_lang == lang_code:
            score -= 0.2
            reasons.append(f"{airline}_native_lang_tolerance")

    score_threshold = CONFIG["WEIRD_CRITERIA"]['score_threshold']
    if 'lufthansa' in text or tweet_lang == 'de':
        score_threshold = 1.5
    if score >= score_threshold:
        reasons.append(f"score={score:.2f}")
        return True, reasons
    return False, []

def contains_airline_name_or_is_quote_or_reply(tweet: Dict) -> bool:
    """Check if tweet is relevant: mentions airline (regex or fuzzy), contains airline keywords, or is a quote/reply."""
    text = extract_full_text(tweet).lower()
    is_quote = tweet.get('is_quote_status', False)
    is_reply = tweet.get('in_reply_to_status_id') is not None

    # Check regex patterns for airline names
    if any(pattern.search(text) for pattern in airline_patterns):
        return True

    # Check fuzzy matching for airline names
    for airline in AIRLINE_NAMES:
        if fuzz.partial_ratio(airline, text) > FUZZY_THRESHOLDS.get(airline, 80):
            print(f"Tweet ID {tweet.get('id', 'unknown')} matched airline '{airline}' via fuzzy matching")
            return True

    # Check for airline-related keywords
    if any(keyword in text for keyword in AIRLINE_KEYWORDS):
        return True

    # Check if tweet is a quote or reply
    if is_quote or is_reply:
        return True

    return False

def process_file(filename: str) -> Tuple[str, Dict, List]:
    """Process JSON file and save cleaned tweets."""
    print(f"Processing file: {filename}")
    json_file_path = os.path.join(json_dir_path, filename)
    output_file_path = os.path.join(output_dir_path, f"cleaned_{filename}")
    batch_size = 10000
    cleaned_tweets = []
    stats = {
        'total_lines': 0, 'valid_lines': 0, 'invalid_lines': 0, 'filtered_lines': 0,
        'weird_accounts': 0, 'irrelevant': 0, 'spam': 0, 'tweets_saved': 0, 'reasons_filtered': []
    }

    if not os.path.exists(json_file_path):
        print(f"File {filename} does not exist")
        return filename, stats, []

    try:
        with open(json_file_path, 'r', encoding='utf-8') as file, open(output_file_path, 'w', encoding='utf-8') as output_file:
            for line in file:
                stats['total_lines'] += 1
                try:
                    tweet = json.loads(line.strip())
                    is_valid, reason = validate_tweet(tweet)
                    if not is_valid:
                        stats['invalid_lines'] += 1
                        stats['reasons_filtered'].append(reason)
                        continue
                    stats['valid_lines'] += 1

                    if contains_airline_name_or_is_quote_or_reply(tweet):
                        cleaned_tweet, error = extract_relevant_info(tweet, filename)
                        if error or not cleaned_tweet:
                            stats['invalid_lines'] += 1
                            stats['reasons_filtered'].append(error or "failed_extraction")
                            continue
                        if cleaned_tweet['is_spam']:
                            stats['spam'] += 1
                            stats['filtered_lines'] += 1
                            stats['reasons_filtered'].append("spam_keywords_detected")
                            continue
                        is_weird, reasons = is_weird_account(cleaned_tweet['user'], tweet.get('created_at'), tweet)
                        if is_weird:
                            stats['weird_accounts'] += 1
                            stats['filtered_lines'] += 1
                            stats['reasons_filtered'].extend(reasons)
                        else:
                            if cleaned_tweet['text']:
                                cleaned_tweets.append(cleaned_tweet)
                                if len(cleaned_tweets) >= batch_size:
                                    output_file.writelines(json.dumps(t, ensure_ascii=False) + '\n' for t in cleaned_tweets)
                                    output_file.flush()
                                    cleaned_tweets = []
                    else:
                        stats['irrelevant'] += 1
                        stats['filtered_lines'] += 1
                        stats['reasons_filtered'].append("irrelevant_content")
                except json.JSONDecodeError as e:
                    stats['invalid_lines'] += 1
                    stats['reasons_filtered'].append(f"JSON decode error: {str(e)}")
                except Exception as e:
                    stats['invalid_lines'] += 1
                    stats['reasons_filtered'].append(f"Unexpected error: {str(e)}")

            if cleaned_tweets:
                output_file.writelines(json.dumps(t, ensure_ascii=False) + '\n' for t in cleaned_tweets)
                output_file.flush()

    except PermissionError as e:
        print(f"Permission error processing {filename}: {e}")
        return filename, stats, []
    except Exception as e:
        print(f"Error processing {filename}: {e}")
        return filename, stats, []

    stats['tweets_saved'] = stats['valid_lines'] - stats['filtered_lines']
    return filename, stats, cleaned_tweets

def process_file_wrapper(args: Tuple[str]) -> Tuple[str, Dict, List]:
    """Wrapper for ProcessPoolExecutor."""
    return process_file(args[0])

def plot_summary_stats(total_processed, total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_saved):
    """Plot bar chart of tweet processing summary."""
    if total_processed == 0:
        print("No tweets processed, skipping plot generation.")
        return

    categories = ["Valid", "Invalid", "Weird", "Irrelevant", "Spam", "Saved"]
    values = [total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_saved]
    colors = ['#4CAF50', '#F44336', '#FFC107', '#2196F3', '#FF5722', '#9C27B0']

    plt.figure(figsize=(10, 6))
    bars = plt.bar(categories, values, color=colors, edgecolor='black')
    plt.title("Tweet Processing Summary", fontsize=14)
    plt.xlabel("Tweet Category", fontsize=12)
    plt.ylabel("Number of Tweets", fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Format Y-axis with thousands separator
    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))

    # Add count and percentage on top of bars
    for bar, value in zip(bars, values):
        height = bar.get_height()
        percentage = (value / total_processed * 100) if total_processed > 0 else 0
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            height + max(values) * 0.02,
            f'{value:,}\n({percentage:.2f}%)',
            ha='center',
            va='bottom',
            fontsize=10
        )

    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir_path, 'summary_bar_chart.png'))
    plt.close()

def main():
    """Main function to process JSON files."""
    json_files = [f for f in os.listdir(json_dir_path) if f.endswith('.json')]
    if not json_files:
        print("No JSON files found.")
        return

    print(f"Found {len(json_files)} JSON files to process.")
    skipped_files, empty_files, file_stats = [], [], []

    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_file_wrapper, [(f,) for f in json_files]))

    for filename, stats, _ in results:
        file_stats.append(stats)
        if stats['tweets_saved'] == 0:
            empty_files.append(filename)
        if stats['total_lines'] == stats['invalid_lines']:
            skipped_files.append(filename)

    total_processed = sum(s['total_lines'] for s in file_stats)
    total_valid = sum(s['valid_lines'] for s in file_stats)
    total_invalid = sum(s['invalid_lines'] for s in file_stats)
    total_filtered = sum(s['filtered_lines'] for s in file_stats)
    total_weird = sum(s['weird_accounts'] for s in file_stats)
    total_irrelevant = sum(s['irrelevant'] for s in file_stats)
    total_spam = sum(s['spam'] for s in file_stats)
    total_saved = sum(s['tweets_saved'] for s in file_stats)

    
    def calc_percentage(count: int) -> str:
        """Calculate percentages."""
        return f"{count} ({count / total_processed * 100:.2f}%)" if total_processed > 0 else f"{count} (0.00%)"

    summary = (
        f"=== Overall Summary ===\n"
        f"Note: 'Valid' tweets pass initial validation. 'Weird', 'Irrelevant', and 'Spam' are filtered from 'Valid' tweets.\n"
        f"'Saved' tweets are 'Valid' tweets that pass all filters (not Weird, Irrelevant, or Spam).\n"
        f"Total files processed: {len(json_files)}\n"
        f"Files skipped due to read errors: {len(skipped_files)}\n"
        f"Files with no relevant tweets: {len(empty_files)}\n"
        f"Total tweets processed: {calc_percentage(total_processed)}\n"
        f"  - Valid tweets: {calc_percentage(total_valid)}\n"
        f"  - Invalid tweets removed: {calc_percentage(total_invalid)}\n"
        f"  - Tweets removed due to weird accounts: {calc_percentage(total_weird)}\n"
        f"  - Tweets removed as irrelevant: {calc_percentage(total_irrelevant)}\n"
        f"  - Tweets removed as spam: {calc_percentage(total_spam)}\n"
        f"Total tweets removed (invalid + filtered): {calc_percentage(total_invalid + total_filtered)}\n"
        f"Total tweets kept (saved): {calc_percentage(total_saved)}\n"
    )
    print(summary)

    # Save summary to file
    summary_file_path = os.path.join(plots_dir_path, 'summary.txt')
    with open(summary_file_path, 'w', encoding='utf-8') as summary_file:
        summary_file.write(summary)

    # Plot summary statistics
    plot_summary_stats(total_processed, total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_saved)

if __name__ == "__main__":
    main()
