import json
import os

# Input directory containing JSON files
json_dir_path = r"C:\Users\20243314\OneDrive - TU Eindhoven\Desktop\Quartile 4\DBL Data Challenge\Dataset"

# Output directory to store cleaned JSON files
output_dir_path = r"C:\Users\20243314\OneDrive - TU Eindhoven\Desktop\Quartile 4\DBL Data Challenge\Cleaned Dataset"

# Create the output directory if it doesn't exist
os.makedirs(output_dir_path, exist_ok=True)

def extract_relevant_info(tweet):

    urls = tweet.get('entities', {}).get('urls', [])
    cleaned_urls = [{'url': url.get('url'), 'display_url': url.get('display_url')} for url in urls]

    return {
        'created_at': tweet.get('created_at'),
        'id': tweet.get('id'),
        'text': tweet.get('text') or tweet.get('extended_tweet', {}).get('full_text'),
        'lang': tweet.get('lang'),
        'retweet_count': tweet.get('retweet_count'),
        'favorite_count': tweet.get('favorite_count'),
        'user': {
            'id': tweet.get('user', {}).get('id'),
            'screen_name': tweet.get('user', {}).get('screen_name'),
            'name': tweet.get('user', {}).get('name'),
            'followers_count': tweet.get('user', {}).get('followers_count'),
            'friends_count': tweet.get('user', {}).get('friends_count'),
            'favourites_count': tweet.get('user', {}).get('favourites_count'),
            'statuses_count': tweet.get('user', {}).get('statuses_count'),
            'verified': tweet.get('user', {}).get('verified'),
            'location': tweet.get('user', {}).get('location'),
            'created_at': tweet.get('user', {}).get('created_at')
        },
        'entities': {
            'hashtags': tweet.get('entities', {}).get('hashtags'),
            'user_mentions': tweet.get('entities', {}).get('user_mentions'),
            'urls': cleaned_urls,
            'symbols': tweet.get('entities', {}).get('symbols'),
        }
    }

# Loop through all JSON files in the directory
for filename in os.listdir(json_dir_path):
    if filename.endswith('.json'):
        json_file_path = os.path.join(json_dir_path, filename)
        output_file_path = os.path.join(output_dir_path, f"cleaned_{filename}")

        cleaned_tweets = []  # Collect all cleaned tweets here

        with open(json_file_path, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    tweet = json.loads(line.strip())  # Parse each line as JSON
                    cleaned_tweet = extract_relevant_info(tweet)
                    cleaned_tweets.append(cleaned_tweet)
                except json.JSONDecodeError as e:
                    print(f"Skipping invalid JSON in {filename}: {e}")

        # Now write the full list to one output file as a proper JSON array
        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            json.dump(cleaned_tweets, output_file, ensure_ascii=False, indent=2)
