import json
import os

# Input directory containing JSON files
json_dir_path = "C:\\Users\\nicol\\OneDrive - TU Eindhoven\\Desktop\\Data Challenge\\data"

# Output directory to store cleaned JSON files
output_dir_path = "C:\\Users\\nicol\\OneDrive - TU Eindhoven\\Desktop\\Data Challenge\\datacleanedtweets"

# Create the output directory if it doesn't exist
os.makedirs(output_dir_path, exist_ok=True)

# List of airline company names
airline_names = [
    'KLM', 'AirFrance', 'British_Airways', 'AmericanAir', 'Lufthansa',
    'AirBerlin', 'AirBerlin assist', 'easyJet', 'RyanAir',
    'SingaporeAir', 'Qantas', 'EtihadAirways', 'VirginAtlantic'
]

def extract_relevant_info(tweet):
    """Extract relevant information from a tweet."""
    # Clean the URLs by removing unnecessary fields
    urls = tweet.get('entities', {}).get('urls', [])
    cleaned_urls = [{'url': url.get('url'), 'display_url': url.get('display_url')} for url in urls]

    return {
        'created_at': tweet.get('created_at'),
        'id': tweet.get('id'),
        'text': tweet.get('text') or tweet.get('extended_tweet', {}).get('full_text'),
        'lang': tweet.get('lang'),
        'retweet_count': tweet.get('retweet_count'),
        'favorite_count': tweet.get('favorite_count'),
        'in_reply_to_status_id': tweet.get('in_reply_to_status_id'),
        'in_reply_to_user_id': tweet.get('in_reply_to_user_id'),
        'in_reply_to_screen_name': tweet.get('in_reply_to_screen_name'),
        'is_quote_status': tweet.get('is_quote_status'),
        'quote_count': tweet.get('quote_count'),
        'reply_count': tweet.get('reply_count'),
        'place': tweet.get('place'),
        'favorited': tweet.get('favorited'),
        'retweeted': tweet.get('retweeted'),
        'user': {
            'id': tweet.get('user', {}).get('id'),
            'screen_name': tweet.get('user', {}).get('screen_name'),
            'name': tweet.get('user', {}).get('name'),
            'followers_count': tweet.get('user', {}).get('followers_count'),
            'friends_count': tweet.get('user', {}).get('friends_count'),
            'favourites_count': tweet.get('user', {}).get('favourites_count'),
            'statuses_count': tweet.get('user', {}).get('statuses_count'),
            'verified': tweet.get('user', {}).get('verified'),
            'location': tweet.get('user', {}).get('location'),
            'time_zone': tweet.get('user', {}).get('time_zone'),
            'created_at': tweet.get('user', {}).get('created_at')
        },
        'entities': {
            'hashtags': tweet.get('entities', {}).get('hashtags'),
            'user_mentions': tweet.get('entities', {}).get('user_mentions'),
            'urls': cleaned_urls,
            'symbols': tweet.get('entities', {}).get('symbols'),
        }
    }

def contains_airline_name_or_is_quote_or_reply(tweet):
    """Check if the tweet contains an airline name, is a quote, or is a reply."""
    text = tweet.get('text', None)
    is_quote = tweet.get('is_quote_status', False)
    is_reply = tweet.get('in_reply_to_status_id') is not None

    if text is not None:
        text = text.lower()
        contains_airline = any(airline.lower() in text for airline in airline_names)
    else:
        contains_airline = False

    return contains_airline or is_quote or is_reply

# Loop through all JSON files in the directory
for filename in os.listdir(json_dir_path):
    if filename.endswith('.json'):
        print(f"Processing file: {filename}")
        json_file_path = os.path.join(json_dir_path, filename)
        output_file_path = os.path.join(output_dir_path, f"cleaned_{filename}")

        # Initialize the list to store cleaned tweets
        cleaned_tweets = []

        # Read and process the file line by line
        with open(json_file_path, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    tweet = json.loads(line.strip())
                    cleaned_tweet = extract_relevant_info(tweet)

                    # Add the tweet to the list if it meets the criteria
                    if contains_airline_name_or_is_quote_or_reply(tweet):
                        cleaned_tweets.append(cleaned_tweet)

                except json.JSONDecodeError as e:
                    print(f"Skipping invalid JSON in {filename}: {e}")

        # Write all cleaned tweets as a JSON array to the output file
        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            json.dump(cleaned_tweets, output_file, ensure_ascii=False, indent=4)
