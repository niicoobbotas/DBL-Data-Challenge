import json  
import os  
import re  
import unicodedata  
from typing import Dict, Tuple, List, Optional 
from dateutil.parser import parse  # pip install python-dateutil
from fuzzywuzzy import fuzz  # pip install fuzzywuzzy
from langdetect import detect  # pip install langdetect
from concurrent.futures import ProcessPoolExecutor  
import matplotlib.pyplot as plt  # pip install matplotlib
from wordcloud import WordCloud  # pip install wordcloud
from collections import Counter  
import hashlib 
import logging  
import concurrent.futures

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Configuration
json_dir_path = "/Users/shibai/Documents/TUE study materials/TUE 2024/JBG030 DBL Data/dbl_data_2025" # Change it, and put your own data folder path(the one about 35gb)
output_dir_path = os.path.join(json_dir_path, "../twitter_data_final_2025") # No need to change
plots_dir_path = os.path.join(output_dir_path, "plots") # No need to change
os.makedirs(output_dir_path, exist_ok=True) # No need to change
os.makedirs(plots_dir_path, exist_ok=True) # No need to change

airline_variations = {
    'klm': [r'\bklm\b', r'\bk\.l\.m\.\b'],
    'airfrance': [r'\bairfrance\b', r'\bair\s*france\b', r'\bAF\b', r'\bAF\d{2,4}\b', r'vol\s+air\s+france'],
    'british_airways': [r'\bbritish_airways\b', r'\bbritish\s*airways\b', r'\bba\b'],
    'americanair': [r'\bamericanair\b', r'\bamerican\s*airlines\b', r'\baa\b'],
    'lufthansa': [r'\blufthansa\b', r'\bdeutsche\s*lufthansa\b', r'\bluftansa\b', r'\blufthanza\b', r'\bLH\b', r'\bLH\d{2,4}\b'],
    'airberlin': [r'\bairberlin\b', r'\bair\s*berlin\b'],
    'airberlin assist': [r'\bairberlin\s*assist\b'],
    'easyjet': [r'\beasyjet\b', r'\beasy\s*jet\b'],
    'ryanair': [r'\bryanair\b', r'\bryan\s*air\b'],
    'singaporeair': [r'\bsingaporeair\b', r'\bsingapore\s*airlines\b'],
    'qantas': [r'\bqantas\b', r'\bQF\b', r'\bQF\d{2,4}\b', r'flying\s+kangaroo'],
    'etihadairways': [r'\betihadairways\b', r'\betihad\s*airways\b'],
    'virginatlantic': [r'\bvirginatlantic\b', r'\bvirgin\s*atlantic\b']
}

AIRLINE_LANG_TOLERANCE = {'lufthansa': 'de', 'airfrance': 'fr', 'klm': 'nl', 'etihadairways': 'ar'}
airline_patterns = [re.compile(pattern, re.IGNORECASE) for patterns in airline_variations.values() for pattern in patterns]
AIRLINE_NAMES = list(airline_variations.keys())
FUZZY_THRESHOLDS = {name: 80 for name in AIRLINE_NAMES}
AIRLINE_KEYWORDS = ['flight', 'airline', 'flew', 'flying', 'plane', 'aviation', 'airport']

AIRLINE_ACCOUNTS = [
    'KLM', 'AirFrance', 'British_Airways', 'AmericanAir', 'Lufthansa',
    'AirBerlin', 'AirBerlin assist', 'easyJet', 'RyanAir',
    'SingaporeAir', 'Qantas', 'EtihadAirways', 'VirginAtlantic'
]
AIRLINE_DOMAINS = [
    'klm.com', 'klm.nl', 'klm.co.uk', 'airfrance.com', 'airfrance.fr', 'airfrance.us','britishairways.com', 'aa.com', 'lufthansa.com', 'lufthansa.de', 'book.lufthansa.com',
    'airberlin.com', 'easyjet.com', 'ryanair.com', 'singaporeair.com', 'qantas.com', 'etihad.com', 'virginatlantic.com'
]

CONFIG = {
     # I set threshold 1.5 to define weird accounts, you can modify it to compare different test result
    "WEIRD_CRITERIA": {"max_age_days": 30, "min_followers": 10, "score_threshold": 1.5},  
    
     #You can find more words here:https://lix-it.com/blog/spam-trigger-words/#:~:text=Some%20common%20spam%20trigger%20words,entice%20the%20recipient%20into%20taking
    "spam_keywords": [
        "earn money", "limited time offer", "make money fast", "special promo code", "free tokens", "cryptocurrency", "win prize", "click here", "exclusive deal",
        "follow and win", "get rich quick", "discount code", "free gift", "Buy 1 Get 1 Free", "earn money", "crypto", "bitcoin", "Call now!" "Sign up free today",
        "marketing solutions", "Increase sales"
    ],      
     
     # This list defines keywords that indicate user-reported scams or service issues, exempting such tweets from spam filtering.
    "scam_report_keywords": [
        "scam", "phishing", "fake account", "fraud", "harassment", "assault", "violence", "homophobic", "discrimination", "booking reference", "delay"
    ],

    # These regex patterns identify typical spam structures such as short URLs, discount offers, excessive links, and mass mentions to flag tweets as spam.
    "spam_patterns": [
        {"pattern": r'https?://(bit\.ly|tinyurl\.com|goo\.gl|t\.co)/\S+', "label": "short_url"},
        {"pattern": r'\b\d{1,2}%?\s*off\b', "label": "discount_offer"},
        {"pattern": r'(https?://\S+\s*){3,}', "label": "multiple_urls"},
        {"pattern": r'(@\w+\s*){4,}', "label": "excessive_mentions"}
    ],

    # The minimum lenght of tweet is 5, you can adjust the length
    "min_text_length_for_langdetect": 5
}

class TweetFilter:
    """Handles filtering logic for tweets based on spam, weird accounts, and duplicates."""
    
    def __init__(self, config: Dict, airline_patterns: List[re.Pattern], airline_accounts: List[str], airline_domains: List[str]):
        """Initialize TweetFilter with configuration and patterns."""
        self.config = config
        self.airline_patterns = airline_patterns
        self.airline_accounts = airline_accounts
        self.airline_domains = airline_domains
        self.spam_patterns = [(re.compile(item["pattern"], re.IGNORECASE), item["label"]) 
                             for item in config["spam_patterns"]]
        self.seen_texts = set()

    def _check_airline_retweet(self, tweet: Dict) -> bool:
        """Check if tweet is a retweet from an airline account."""
        if 'retweeted_status' in tweet:
            retweet_user = tweet['retweeted_status'].get('user', {}).get('screen_name', '')
            return retweet_user in self.airline_accounts
        return False

    def _check_airline_url(self, tweet: Dict) -> bool:
        """Check if tweet contains airline-related URLs."""
        if 'entities' in tweet and 'urls' in tweet['entities']:
            for url in tweet['entities']['urls']:
                expanded_url = url.get('expanded_url', '').lower()
                if expanded_url and any(domain in expanded_url for domain in self.airline_domains):
                    return True
        return False

    def is_spam(self, tweet: Dict, cleaned_text: str) -> Tuple[bool, List[str]]:
        """Check if a tweet is spam, with exemptions for airline-related content."""
        spam_reasons = []
        is_airline_retweet = self._check_airline_retweet(tweet)
        is_airline_url = self._check_airline_url(tweet)
        is_scam_report = any(keyword in cleaned_text for keyword in self.config["scam_report_keywords"])
        is_airline_mention = any(pattern.search(cleaned_text) for pattern in self.airline_patterns)

        if any([is_airline_retweet, is_airline_url, is_scam_report, is_airline_mention]):
            spam_reasons.append("exempt: airline_related")
            return False, spam_reasons

        matched_keyword = next((k for k in self.config["spam_keywords"] if k in cleaned_text), None)
        if matched_keyword:
            spam_reasons.append(f"keyword: {matched_keyword}")
        for pattern, label in self.spam_patterns:
            if pattern.search(cleaned_text):
                spam_reasons.append(f"pattern: {label}")
        
        return bool(spam_reasons), spam_reasons

    def is_weird_account(self, user: Dict, tweet_created_at: str, tweet: Dict) -> Tuple[bool, List[str]]:
        """Check if an account is suspicious based on criteria."""
        if not user or not tweet:
            return True, ["missing_user_info_or_tweet"]
        score, reasons = 0.0, []
        followers = user.get('followers_count', 0)
        friends = user.get('friends_count', 0)
        statuses = user.get('statuses_count', 0)
        screen_name = user.get('screen_name', '')
        verified = user.get('verified', False)
        description = user.get('description', '')
        profile_image = user.get('profile_image_url', '')

        if followers < self.config["WEIRD_CRITERIA"]['min_followers'] and statuses > 5000:
            score += 0.8
            reasons.append("low_followers_high_activity")
        if followers < self.config["WEIRD_CRITERIA"]['min_followers'] and friends > 500:
            score += 0.6 if not verified else 0.2
            reasons.append("spammy_follow_ratio")
        if statuses < 10:
            score += 0.5
            reasons.append("very_low_tweet_count")
        if re.fullmatch(r'[a-zA-Z]*\d{5,}', screen_name):
            score += 0.4
            reasons.append("autogen_screen_name")
        if not verified:
            score += 0.1
            reasons.append("not_verified")
        if not description:
            score += 0.3
            reasons.append("empty_profile_description")
        if profile_image and 'default_profile' in profile_image.lower():
            score += 0.5
            reasons.append("default_profile_image")

        created_at = user.get('created_at', '')
        if created_at and tweet_created_at:
            try:
                user_created = parse(created_at, fuzzy=False)
                tweet_created = parse(tweet_created_at, fuzzy=False)
                age_days = max((tweet_created - user_created).days, 1)
                if age_days < self.config["WEIRD_CRITERIA"]['max_age_days'] and statuses > 500 and followers < self.config["WEIRD_CRITERIA"]['min_followers']:
                    score += 0.7
                    reasons.append("new_active_no_followers")
                if statuses / age_days > 100:
                    score += 0.4
                    reasons.append("high_tweet_frequency")
            except (ValueError, TypeError):
                reasons.append("skipped_age_check_invalid_date")

        text = extract_full_text(tweet).lower()
        tweet_lang = tweet.get('lang', '')
        if not tweet_lang or tweet_lang == 'und':
            if len(text) >= self.config["min_text_length_for_langdetect"]:
                try:
                    tweet_lang = detect(text)
                except Exception:
                    tweet_lang = ''
        for airline, lang_code in AIRLINE_LANG_TOLERANCE.items():
            if airline in text and tweet_lang == lang_code:
                score -= 0.2
                reasons.append(f"{airline}_native_lang_tolerance")

        score_threshold = self.config["WEIRD_CRITERIA"]['score_threshold']
        if 'lufthansa' in text or tweet_lang == 'de':
            score_threshold = 1.5
        if score >= score_threshold:
            reasons.append(f"score={score:.2f}")
            return True, reasons
        return False, []

    def is_duplicate(self, user_id: str, text: str) -> bool:
        """Check if a tweet is a duplicate by the same user."""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        user_text_pair = (user_id, text_hash)
        if user_text_pair in self.seen_texts:
            return True
        self.seen_texts.add(user_text_pair)
        return False

class TweetProcessor:
    """Processes and cleans tweet data, applying filters and generating statistics."""
    
    def __init__(self, json_dir: str, output_dir: str, plots_dir: str, config: Dict, airline_patterns: List[re.Pattern]):
        """Initialize TweetProcessor with directories and configuration."""
        self.json_dir = json_dir
        self.output_dir = output_dir
        self.plots_dir = plots_dir
        self.config = config
        self.airline_patterns = airline_patterns
        self.filter = TweetFilter(config, airline_patterns, AIRLINE_ACCOUNTS, AIRLINE_DOMAINS)
        self.stats = {
            'total_lines': 0, 'valid_lines': 0, 'invalid_lines': 0, 'filtered_lines': 0,
            'weird_accounts': 0, 'irrelevant': 0, 'spam': 0, 'duplicates': 0, 'tweets_saved': 0,
            'reasons_filtered': []
        }

    def process_single_tweet(self, tweet: Dict, filename: str) -> Tuple[Optional[Dict], str]:
        """Process a single tweet, applying validation and filters."""
        is_valid, reason = validate_tweet(tweet)
        if not is_valid:
            self.stats['invalid_lines'] += 1
            self.stats['reasons_filtered'].append(reason)
            logging.debug(f"Invalid tweet in {filename}: {reason}")
            return None, reason
        else:
            self.stats['valid_lines'] += 1
            logging.debug(f"Valid tweet in {filename}, valid_lines={self.stats['valid_lines']}")

        if not contains_airline_name_or_is_quote_or_reply(tweet):
            self.stats['irrelevant'] += 1
            self.stats['filtered_lines'] += 1
            self.stats['reasons_filtered'].append("irrelevant_content")
            return None, "irrelevant_content"

        cleaned_tweet, error = extract_relevant_info(tweet, filename)
        if error or not cleaned_tweet or not cleaned_tweet['text']:
            self.stats['invalid_lines'] += 1
            self.stats['reasons_filtered'].append(error or "failed_extraction")
            logging.debug(f"Failed extraction in {filename}: {error or 'failed_extraction'}")
            return None, error or "failed_extraction"

        is_spam, spam_reasons = self.filter.is_spam(tweet, cleaned_tweet['text'])
        if is_spam:
            self.stats['spam'] += 1
            self.stats['filtered_lines'] += 1
            self.stats['reasons_filtered'].append("spam_keywords_detected")
            return None, "spam_keywords_detected"

        is_weird, reasons = self.filter.is_weird_account(cleaned_tweet['user'], tweet.get('created_at'), tweet)
        if is_weird:
            self.stats['weird_accounts'] += 1
            self.stats['filtered_lines'] += 1
            self.stats['reasons_filtered'].extend(reasons)
            return None, "weird_account"

        if self.filter.is_duplicate(cleaned_tweet['user']['id_str'], cleaned_tweet['text']):
            self.stats['duplicates'] += 1
            self.stats['filtered_lines'] += 1
            self.stats['reasons_filtered'].append("duplicate_tweet_by_user")
            return None, "duplicate_tweet_by_user"

        self.stats['tweets_saved'] += 1
        return cleaned_tweet, ""

    def process_file(self, filename: str) -> Tuple[str, Dict, List, List]:
        """Process a single JSON file and save cleaned tweets."""
        logging.info(f"Processing file: {filename}")
        json_file_path = os.path.join(self.json_dir, filename)
        output_file_path = os.path.join(self.output_dir, f"cleaned_{filename}")
        spam_log_path = os.path.join(self.output_dir, "spam_review.json")
        cleaned_tweets = []
        spam_tweets = []
        all_spam_reasons = []
        max_spam_samples = 100

        if not os.path.exists(json_file_path):
            logging.error(f"File {filename} does not exist")
            return filename, self.stats, [], []

        try:
            with open(json_file_path, 'r', encoding='utf-8') as file, \
                 open(output_file_path, 'w', encoding='utf-8') as output_file:
                for line in file:
                    self.stats['total_lines'] += 1
                    try:
                        tweet = json.loads(line.strip())
                        cleaned_tweet, reason = self.process_single_tweet(tweet, filename)
                        if cleaned_tweet:
                            cleaned_tweets.append(cleaned_tweet)
                            if len(cleaned_tweets) >= 10000:
                                output_file.writelines(json.dumps(t, ensure_ascii=False) + '\n' 
                                                     for t in cleaned_tweets)
                                cleaned_tweets = []
                        elif reason == "spam_keywords_detected" and len(spam_tweets) < max_spam_samples:
                            spam_tweets.append({
                                'id': tweet.get('id'),
                                'text': extract_full_text(tweet),
                                'spam_reasons': self.filter.is_spam(tweet, clean_text(extract_full_text(tweet), tweet, filename)[0])[1]
                            })
                            all_spam_reasons.extend(spam_tweets[-1]['spam_reasons'])
                    except json.JSONDecodeError as e:
                        self.stats['invalid_lines'] += 1
                        self.stats['reasons_filtered'].append(f"JSON decode error: {str(e)}")
                        logging.debug(f"JSON decode error in {filename}: {str(e)}")
                    except Exception as e:
                        self.stats['invalid_lines'] += 1
                        self.stats['reasons_filtered'].append(f"Unexpected error: {str(e)}")
                        logging.debug(f"Unexpected error in {filename}: {str(e)}")

                if cleaned_tweets:
                    output_file.writelines(json.dumps(t, ensure_ascii=False) + '\n' 
                                         for t in cleaned_tweets)

            if spam_tweets:
                with open(spam_log_path, 'a', encoding='utf-8') as spam_file:
                    for spam_tweet in spam_tweets:
                        spam_file.write(json.dumps(spam_tweet, ensure_ascii=False) + '\n')

        except Exception as e:
            logging.error(f"Error processing {filename}: {e}")
            return filename, self.stats, [], []

        return filename, self.stats, cleaned_tweets, all_spam_reasons

def clean_text(text: str, tweet: Dict = None, filename: str = None) -> Tuple[str, bool, List[str]]:
    """Clean and normalize text, detect spam with exemptions for airline retweets, URLs, and scam reports."""
    if not text:
        return "", False, []
    text = unicodedata.normalize('NFKC', text.lower()).strip()
    text = ' '.join(text.split())
    return text, False, []

def extract_full_text(tweet: Dict) -> str:
    """Extract full text, handling retweets."""
    source = tweet.get('retweeted_status', tweet)
    return source.get('full_text') or source.get('extended_tweet', {}).get('full_text') or source.get('text') or ''

def validate_tweet(tweet: Dict) -> Tuple[bool, str]:
    """Validate required tweet fields."""
    required_fields = ['created_at', 'id', 'text', 'user']
    missing = [f for f in required_fields if f not in tweet or tweet[f] is None]
    if missing:
        return False, f"Missing fields: {', '.join(missing)}"
    if not tweet['user'].get('id_str'):
        return False, "Missing user id_str"
    return True, ""

def extract_relevant_info(tweet: Dict, filename: str = None) -> Tuple[Dict, None]:
    """Extract and clean tweet data."""
    text, is_spam, spam_reasons = clean_text(extract_full_text(tweet), tweet, filename)
    urls = tweet.get('entities', {}).get('urls', [])
    cleaned_urls = [{'url': u.get('url'), 'display_url': u.get('display_url')} for u in urls]
    return {
        'created_at': tweet.get('created_at'),
        'id': tweet.get('id'),
        'text': text,
        'is_spam': is_spam,
        'spam_reasons': spam_reasons,
        'lang': tweet.get('lang'),
        'retweet_count': tweet.get('retweet_count'),
        'favorite_count': tweet.get('favorite_count'),
        'in_reply_to_status_id': tweet.get('in_reply_to_status_id'),
        'in_reply_to_user_id': tweet.get('in_reply_to_user_id'),
        'in_reply_to_screen_name': tweet.get('in_reply_to_screen_name'),
        'is_quote_status': tweet.get('is_quote_status'),
        'quote_count': tweet.get('quote_count'),
        'reply_count': tweet.get('reply_count'),
        'place': tweet.get('place'),
        'favorited': tweet.get('favorited'),
        'retweeted': tweet.get('retweeted'),
        'user': {
            'id': tweet.get('user', {}).get('id'),
            'id_str': tweet.get('user', {}).get('id_str'),
            'screen_name': tweet.get('user', {}).get('screen_name'),
            'name': tweet.get('user', {}).get('name'),
            'followers_count': tweet.get('user', {}).get('followers_count'),
            'friends_count': tweet.get('user', {}).get('friends_count'),
            'favourites_count': tweet.get('user', {}).get('favourites_count'),
            'statuses_count': tweet.get('user', {}).get('statuses_count'),
            'verified': tweet.get('user', {}).get('verified'),
            'location': tweet.get('user', {}).get('location'),
            'time_zone': tweet.get('user', {}).get('time_zone'),
            'created_at': tweet.get('user', {}).get('created_at')
        },
        'entities': {
            'hashtags': tweet.get('entities', {}).get('hashtags'),
            'user_mentions': tweet.get('entities', {}).get('user_mentions'),
            'urls': cleaned_urls,
            'symbols': tweet.get('entities', {}).get('symbols')
        }
    }, None

def contains_airline_name_or_is_quote_or_reply(tweet: Dict) -> bool:
    """Check if tweet is relevant: mentions airline, contains keywords, or is a quote/reply."""
    text = extract_full_text(tweet).lower()
    is_quote = tweet.get('is_quote_status', False)
    is_reply = tweet.get('in_reply_to_status_id') is not None

    if any(pattern.search(text) for pattern in airline_patterns):
        return True

    for airline in AIRLINE_NAMES:
        if fuzz.partial_ratio(airline, text) > FUZZY_THRESHOLDS.get(airline, 80):
            logging.info(f"Tweet ID {tweet.get('id', 'unknown')} matched airline '{airline}' via fuzzy matching")
            return True

    if any(keyword in text for keyword in AIRLINE_KEYWORDS):
        return True

    if is_quote or is_reply:
        return True

    return False

def plot_summary_stats(total_processed, total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_duplicates, total_saved):
    """Plot bar chart of tweet processing summary."""
    if total_processed == 0:
        logging.warning("No tweets processed, skipping plot generation.")
        return

    categories = ["Valid", "Invalid", "Weird", "Irrelevant", "Spam", "Duplicates", "Saved"]
    values = [total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_duplicates, total_saved]
    colors = ['#4CAF50', '#F44336', '#FFC107', '#2196F3', '#FF5722', '#FF9800', '#9C27B0']

    plt.figure(figsize=(12, 6))
    bars = plt.bar(categories, values, color=colors, edgecolor='black')
    plt.title("Tweet Processing Summary", fontsize=14)
    plt.xlabel("Tweet Category", fontsize=12)
    plt.ylabel("Number of Tweets", fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))

    for bar, value in zip(bars, values):
        height = bar.get_height()
        percentage = (value / total_processed * 100) if total_processed > 0 else 0
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            height + max(values) * 0.02,
            f'{value:,}\n({percentage:.2f}%)',
            ha='center',
            va='bottom',
            fontsize=10
        )

    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir_path, 'summary_bar_chart.png'))
    plt.close()
    logging.info("Generated summary bar chart")

def plot_spam_reasons(spam_reasons: List[str]):
    """Generate word cloud and bar chart for spam reasons."""
    if not spam_reasons:
        logging.warning("No spam reasons found, skipping visualization.")
        return

    reason_counts = Counter(spam_reasons)
    
    wordcloud = WordCloud(
        width=800, height=400, background_color='white', min_font_size=10
    ).generate_from_frequencies(reason_counts)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Spam Reasons Word Cloud', fontsize=14)
    plt.savefig(os.path.join(plots_dir_path, 'spam_reasons_wordcloud.png'))
    plt.close()
    logging.info("Generated spam reasons word cloud")

    reasons = list(reason_counts.keys())
    counts = list(reason_counts.values())
    plt.figure(figsize=(12, 6))
    bars = plt.bar(range(len(reasons)), counts, color='#FF5722', edgecolor='black')
    plt.title('Spam Reasons Frequency', fontsize=14)
    plt.xlabel('Reason', fontsize=12)
    plt.ylabel('Number of Occurrences', fontsize=12)
    plt.xticks(range(len(reasons)), reasons, rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    for bar, count in zip(bars, counts):
        plt.text(
            bar.get_x() + bar.get_width() / 2, bar.get_height(),
            f'{count:,}', ha='center', va='bottom', fontsize=10
        )

    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir_path, 'spam_reasons_bar_chart.png'))
    plt.close()
    logging.info(f"Generated spam reasons bar chart with {len(reasons)} reasons")

def main():
    """Main function to process JSON files and generate visualizations."""
    json_files = [f for f in os.listdir(json_dir_path) if f.endswith('.json')]
    if not json_files:
        logging.error("No JSON files found.")
        return

    processor = TweetProcessor(json_dir_path, output_dir_path, plots_dir_path, CONFIG, airline_patterns)
    results = []
    
    with ProcessPoolExecutor() as executor:
        future_to_file = {executor.submit(processor.process_file, f): f for f in json_files}
        for future in concurrent.futures.as_completed(future_to_file):
            filename = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                logging.info(f"Completed processing {filename}")
            except Exception as e:
                logging.error(f"Error processing {filename}: {e}")
                results.append((filename, processor.stats, [], []))

    skipped_files, empty_files, file_stats = [], [], []
    all_spam_reasons = []

    for filename, stats, _, spam_reasons in results:
        file_stats.append(stats)
        all_spam_reasons.extend(spam_reasons)
        if stats['tweets_saved'] == 0:
            empty_files.append(filename)
        if stats['total_lines'] == stats['invalid_lines']:
            skipped_files.append(filename)
        # Integrity check
        if stats['total_lines'] != stats['valid_lines'] + stats['invalid_lines']:
            logging.error(f"Integrity check failed for {filename}: total_lines={stats['total_lines']}, "
                         f"valid_lines={stats['valid_lines']}, invalid_lines={stats['invalid_lines']}")

    total_processed = sum(s['total_lines'] for s in file_stats)
    total_valid = sum(s['valid_lines'] for s in file_stats)
    total_invalid = sum(s['invalid_lines'] for s in file_stats)
    total_filtered = sum(s['filtered_lines'] for s in file_stats)
    total_weird = sum(s['weird_accounts'] for s in file_stats)
    total_irrelevant = sum(s['irrelevant'] for s in file_stats)
    total_spam = sum(s['spam'] for s in file_stats)
    total_duplicates = sum(s['duplicates'] for s in file_stats)
    total_saved = sum(s['tweets_saved'] for s in file_stats)

    def calc_percentage(count: int) -> str:
        """Calculate percentages."""
        return f"{count:,} ({count / total_processed * 100:.2f}%)" if total_processed > 0 else f"{count:,} (0.00%)"

    summary = (
        f"=== Overall Summary ===\n"
        f"Note: 'Valid' tweets pass initial validation (have required fields). 'Weird', 'Irrelevant', 'Spam', and 'Duplicates' are filtered from 'Valid' tweets.\n"
        f"'Duplicates' are tweets with identical text posted by the same user (identified by user_id).\n"
        f"'Saved' tweets are 'Valid' tweets that pass all filters (not Weird, Irrelevant, Spam, or Duplicate).\n"
        f"Total files processed: {len(json_files)}\n"
        f"Files skipped due to read errors: {len(skipped_files)}\n"
        f"Files with no relevant tweets: {len(empty_files)}\n"
        f"Total tweets processed: {calc_percentage(total_processed)}\n"
        f"  - Valid tweets: {calc_percentage(total_valid)}\n"
        f"  - Invalid tweets removed: {calc_percentage(total_invalid)}\n"
        f"  - Tweets removed due to weird accounts: {calc_percentage(total_weird)}\n"
        f"  - Tweets removed as irrelevant: {calc_percentage(total_irrelevant)}\n"
        f"  - Tweets removed as spam: {calc_percentage(total_spam)}\n"
        f"  - Tweets removed as duplicates: {calc_percentage(total_duplicates)}\n"
        f"Total tweets removed (invalid + filtered): {calc_percentage(total_invalid + total_filtered)}\n"
        f"Total tweets kept (saved): {calc_percentage(total_saved)}\n"
    )

    logging.info(summary)

    summary_file_path = os.path.join(plots_dir_path, 'summary.txt')
    with open(summary_file_path, 'w', encoding='utf-8') as summary_file:
        summary_file.write(summary)

    plot_summary_stats(total_processed, total_valid, total_invalid, total_weird, total_irrelevant, total_spam, total_duplicates, total_saved)
    plot_spam_reasons(all_spam_reasons)

if __name__ == "__main__":
    main()
